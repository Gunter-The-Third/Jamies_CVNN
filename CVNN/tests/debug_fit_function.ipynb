{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5da858",
   "metadata": {},
   "source": [
    "# Debug CVNN Library Fit Function\n",
    "\n",
    "This notebook systematically debugs matrix dimension issues in the CVNN library's fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f765c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVNN Debug Session - Matrix Dimension Analysis\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Debug CVNN Fit Function - Matrix Dimension Issues\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cvnn import Dense, Sequential\n",
    "from cvnn.activations import complex_sigmoid, complex_sigmoid_backward\n",
    "\n",
    "print(\"CVNN Debug Session - Matrix Dimension Analysis\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9185d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Reproducing the Matrix Dimension Error\n",
      "--------------------------------------------------\n",
      "Input X shape: (2, 2)\n",
      "Target Y shape: (2, 1)\n",
      "\n",
      "Testing Sequential model with fit()...\n",
      "Model created successfully\n",
      "Attempting fit()...\n",
      "Epoch 1/10, Loss: 2.4349\n",
      "Epoch 2/10, Loss: 0.7314\n",
      "Epoch 3/10, Loss: 0.3720\n",
      "Epoch 4/10, Loss: 0.2853\n",
      "Epoch 5/10, Loss: 0.2635\n",
      "Epoch 6/10, Loss: 0.2575\n",
      "Epoch 7/10, Loss: 0.2554\n",
      "Epoch 8/10, Loss: 0.2543\n",
      "Epoch 9/10, Loss: 0.2534\n",
      "Epoch 10/10, Loss: 0.2526\n",
      "Fit completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Reproduce the matrix dimension error using Sequential.fit()\n",
    "print(\"Step 1: Reproducing the Matrix Dimension Error\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create simple test data\n",
    "X_test = np.array([\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0]\n",
    "], dtype=np.float64)\n",
    "\n",
    "Y_test = np.array([\n",
    "    [1.0],\n",
    "    [0.0]\n",
    "], dtype=np.float64)\n",
    "\n",
    "print(f\"Input X shape: {X_test.shape}\")\n",
    "print(f\"Target Y shape: {Y_test.shape}\")\n",
    "\n",
    "# Try to use Sequential with simple activation to trigger the error\n",
    "try:\n",
    "    print(\"\\nTesting Sequential model with fit()...\")\n",
    "    model = Sequential([\n",
    "        Dense(input_dim=2, output_dim=2, real=True, complex=False),\n",
    "        (np.tanh, lambda x, grad: grad * (1 - np.tanh(x)**2)),\n",
    "        Dense(input_dim=2, output_dim=1, real=True, complex=False)\n",
    "    ], real=True)\n",
    "    \n",
    "    print(\"Model created successfully\")\n",
    "    print(\"Attempting fit()...\")\n",
    "    \n",
    "    # This should trigger the matrix dimension error\n",
    "    losses = model.fit(X_test, Y_test, epochs=10, lr=0.1, verbose=True)\n",
    "    print(\"Fit completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR CAUGHT: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    print(\"\\nFull traceback:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f0ead5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 2: Testing Various Scenarios for Matrix Dimension Issues\n",
      "------------------------------------------------------------\n",
      "\n",
      "Testing: Complex-valued single layer\n",
      "X shape: (2, 1), Y shape: (2, 1)\n",
      "✓ SUCCESS - Final loss: 0.2403\n",
      "\n",
      "Testing: Complex with complex_sigmoid\n",
      "X shape: (2, 1), Y shape: (2, 1)\n",
      "✓ SUCCESS - Final loss: 0.7264\n",
      "\n",
      "Testing: Multi-layer real network\n",
      "X shape: (1, 3), Y shape: (1, 1)\n",
      "✗ ERROR: ValueError: operands could not be broadcast together with shapes (1,2) (1,3) \n",
      "\n",
      "Testing: XOR problem with activations\n",
      "X shape: (4, 2), Y shape: (4, 1)\n",
      "✗ ERROR: ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Test different scenarios that might trigger matrix dimension errors\n",
    "print(\"\\n\\nStep 2: Testing Various Scenarios for Matrix Dimension Issues\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def test_scenario(name, model_layers, x_data, y_data):\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    print(f\"X shape: {x_data.shape}, Y shape: {y_data.shape}\")\n",
    "    \n",
    "    try:\n",
    "        model = Sequential(model_layers)\n",
    "        losses = model.fit(x_data, y_data, epochs=5, lr=0.1, verbose=False)\n",
    "        print(f\"✓ SUCCESS - Final loss: {losses[-1]:.4f}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ ERROR: {type(e).__name__}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test Case 1: Complex-valued network\n",
    "X_complex = np.array([[1+1j], [0+1j]], dtype=np.complex128)\n",
    "Y_complex = np.array([[1.0], [0.0]], dtype=np.complex128)\n",
    "\n",
    "test_scenario(\"Complex-valued single layer\", [\n",
    "    Dense(input_dim=1, output_dim=1, complex=True)\n",
    "], X_complex, Y_complex)\n",
    "\n",
    "# Test Case 2: Complex with activation\n",
    "test_scenario(\"Complex with complex_sigmoid\", [\n",
    "    Dense(input_dim=1, output_dim=1, complex=True),\n",
    "    (complex_sigmoid, complex_sigmoid_backward)\n",
    "], X_complex, Y_complex)\n",
    "\n",
    "# Test Case 3: Multiple layers with mismatched dimensions\n",
    "X_mismatch = np.array([[1.0, 2.0, 3.0]], dtype=np.float64)  # 1 sample, 3 features\n",
    "Y_mismatch = np.array([[1.0]], dtype=np.float64)            # 1 sample, 1 output\n",
    "\n",
    "test_scenario(\"Multi-layer real network\", [\n",
    "    Dense(input_dim=3, output_dim=2, real=True, complex=False),\n",
    "    (np.tanh, lambda x, grad: grad * (1 - np.tanh(x)**2)),\n",
    "    Dense(input_dim=2, output_dim=1, real=True, complex=False)\n",
    "], X_mismatch, Y_mismatch)\n",
    "\n",
    "# Test Case 4: XOR problem (from basic_test)\n",
    "X_xor = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]], dtype=np.float64)\n",
    "Y_xor = np.array([[0.0], [1.0], [1.0], [0.0]], dtype=np.float64)\n",
    "\n",
    "test_scenario(\"XOR problem with activations\", [\n",
    "    Dense(input_dim=2, output_dim=2, real=True, complex=False),\n",
    "    (np.tanh, lambda x, grad: grad * (1 - np.tanh(x)**2)),\n",
    "    Dense(input_dim=2, output_dim=1, real=True, complex=False),\n",
    "    (lambda x: 1/(1+np.exp(-x)), lambda x, grad: grad * (1/(1+np.exp(-x))) * (1 - 1/(1+np.exp(-x))))\n",
    "], X_xor, Y_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26af552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 3: Analyzing the Root Cause of Matrix Dimension Errors\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Debugging XOR case that failed...\n",
      "Input X shape: (2, 2)\n",
      "Layer 1 output z1 shape: (2, 2)\n",
      "After tanh a1 shape: (2, 2)\n",
      "Layer 2 output z2 shape: (2, 1)\n",
      "Final output a2 shape: (2, 1)\n",
      "Target Y shape: (2, 1)\n",
      "\n",
      "Loss: 0.4565\n",
      "Initial gradient shape: (2, 1)\n",
      "\n",
      "--- BACKWARD PASS ANALYSIS ---\n",
      "Sigmoid backward input (z2): (2, 1)\n",
      "Sigmoid backward grad_output: (2, 1)\n",
      "After sigmoid derivative grad_z2: (2, 1)\n",
      "\n",
      "Layer 2 backward:\n",
      "  Layer 2 cached input (a1): (2, 2)\n",
      "  Layer 2 weights W: (2, 1)\n",
      "  Incoming gradient: (2, 1)\n",
      "  Layer 2 backward successful, grad_a1: (2, 2)\n",
      "\n",
      "Tanh backward:\n",
      "  Tanh input was z1: (2, 2)\n",
      "  Incoming gradient grad_a1: (2, 2)\n",
      "  After tanh derivative grad_z1: (2, 2)\n",
      "\n",
      "Layer 1 backward:\n",
      "  Layer 1 cached input (X): (2, 2)\n",
      "  Layer 1 weights W: (2, 2)\n",
      "  Incoming gradient: (2, 2)\n",
      "  Layer 1 backward successful, grad_x: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Deep Dive into the Matrix Dimension Issues\n",
    "print(\"\\n\\nStep 3: Analyzing the Root Cause of Matrix Dimension Errors\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Let's examine the Sequential.backward method step by step\n",
    "def debug_sequential_backward():\n",
    "    print(\"\\nDebugging XOR case that failed...\")\n",
    "    \n",
    "    # Recreate the failing case\n",
    "    X_debug = np.array([[0.0, 0.0], [0.0, 1.0]], dtype=np.float64)  # Smaller for debugging\n",
    "    Y_debug = np.array([[0.0], [1.0]], dtype=np.float64)\n",
    "    \n",
    "    # Create layers manually to inspect\n",
    "    layer1 = Dense(input_dim=2, output_dim=2, real=True, complex=False)\n",
    "    tanh_activation = (np.tanh, lambda x, grad: grad * (1 - np.tanh(x)**2))\n",
    "    layer2 = Dense(input_dim=2, output_dim=1, real=True, complex=False)\n",
    "    sigmoid_activation = (lambda x: 1/(1+np.exp(-x)), lambda x, grad: grad * (1/(1+np.exp(-x))) * (1 - 1/(1+np.exp(-x))))\n",
    "    \n",
    "    # Manual forward pass\n",
    "    print(f\"Input X shape: {X_debug.shape}\")\n",
    "    \n",
    "    # Layer 1 forward\n",
    "    z1 = layer1.forward(X_debug)\n",
    "    print(f\"Layer 1 output z1 shape: {z1.shape}\")\n",
    "    \n",
    "    # Tanh activation\n",
    "    a1 = np.tanh(z1)\n",
    "    print(f\"After tanh a1 shape: {a1.shape}\")\n",
    "    \n",
    "    # Layer 2 forward\n",
    "    z2 = layer2.forward(a1)\n",
    "    print(f\"Layer 2 output z2 shape: {z2.shape}\")\n",
    "    \n",
    "    # Sigmoid activation\n",
    "    a2 = 1/(1+np.exp(-z2))\n",
    "    print(f\"Final output a2 shape: {a2.shape}\")\n",
    "    print(f\"Target Y shape: {Y_debug.shape}\")\n",
    "    \n",
    "    # Calculate loss and gradients\n",
    "    loss = np.mean(np.abs(a2 - Y_debug) ** 2)\n",
    "    grad_output = 2 * (a2 - Y_debug) / Y_debug.shape[0]\n",
    "    print(f\"\\nLoss: {loss:.4f}\")\n",
    "    print(f\"Initial gradient shape: {grad_output.shape}\")\n",
    "    \n",
    "    # Now trace the backward pass step by step\n",
    "    print(\"\\n--- BACKWARD PASS ANALYSIS ---\")\n",
    "    \n",
    "    # Sigmoid backward\n",
    "    sigmoid_input = z2  # This is what sigmoid saw\n",
    "    sigmoid_output = a2  # This is what sigmoid produced\n",
    "    \n",
    "    print(f\"Sigmoid backward input (z2): {sigmoid_input.shape}\")\n",
    "    print(f\"Sigmoid backward grad_output: {grad_output.shape}\")\n",
    "    \n",
    "    # Apply sigmoid derivative\n",
    "    grad_z2 = grad_output * sigmoid_output * (1 - sigmoid_output)\n",
    "    print(f\"After sigmoid derivative grad_z2: {grad_z2.shape}\")\n",
    "    \n",
    "    # Layer 2 backward\n",
    "    print(f\"\\nLayer 2 backward:\")\n",
    "    print(f\"  Layer 2 cached input (a1): {layer2.x_cache.shape}\")\n",
    "    print(f\"  Layer 2 weights W: {layer2.W.shape}\")\n",
    "    print(f\"  Incoming gradient: {grad_z2.shape}\")\n",
    "    \n",
    "    # This is where the error likely occurs\n",
    "    try:\n",
    "        grad_a1 = layer2.backward(grad_z2, lr=0.1)\n",
    "        print(f\"  Layer 2 backward successful, grad_a1: {grad_a1.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Layer 2 backward FAILED: {e}\")\n",
    "        \n",
    "        # Let's manually compute what should happen\n",
    "        print(f\"  Manual computation:\")\n",
    "        print(f\"    grad_z2 @ layer2.W.T = {grad_z2.shape} @ {layer2.W.T.shape}\")\n",
    "        print(f\"    Expected result shape: {(grad_z2.shape[0], layer2.W.T.shape[1])}\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Continue with tanh backward if layer2 succeeded\n",
    "    print(f\"\\nTanh backward:\")\n",
    "    print(f\"  Tanh input was z1: {z1.shape}\")\n",
    "    print(f\"  Incoming gradient grad_a1: {grad_a1.shape}\")\n",
    "    \n",
    "    # Apply tanh derivative\n",
    "    grad_z1 = grad_a1 * (1 - np.tanh(z1)**2)\n",
    "    print(f\"  After tanh derivative grad_z1: {grad_z1.shape}\")\n",
    "    \n",
    "    # Layer 1 backward\n",
    "    print(f\"\\nLayer 1 backward:\")\n",
    "    print(f\"  Layer 1 cached input (X): {layer1.x_cache.shape}\")\n",
    "    print(f\"  Layer 1 weights W: {layer1.W.shape}\")\n",
    "    print(f\"  Incoming gradient: {grad_z1.shape}\")\n",
    "    \n",
    "    try:\n",
    "        grad_x = layer1.backward(grad_z1, lr=0.1)\n",
    "        print(f\"  Layer 1 backward successful, grad_x: {grad_x.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Layer 1 backward FAILED: {e}\")\n",
    "\n",
    "debug_sequential_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903bbcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 4: Investigating Sequential.backward Implementation\n",
      "------------------------------------------------------------\n",
      "Creating Sequential model that fails...\n",
      "Model layers: 4\n",
      "  Layer 0: Dense layer with weights (2, 2)\n",
      "  Layer 1: Activation function\n",
      "  Layer 2: Dense layer with weights (2, 1)\n",
      "  Layer 3: Activation function\n",
      "\n",
      "Trying forward pass...\n",
      "Forward pass successful, output shape: (4, 1)\n",
      "Cache length: 4\n",
      "  Cache 0: layer\n",
      "    Cached input shape: (4, 2)\n",
      "  Cache 1: activation\n",
      "  Cache 2: layer\n",
      "    Cached input shape: (4, 2)\n",
      "  Cache 3: activation\n",
      "\n",
      "Trying backward pass...\n",
      "Initial gradient shape: (4, 1)\n",
      "Backward pass failed: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)\n",
      "Detailed traceback:\n",
      "\n",
      "--- DEBUGGING SEQUENTIAL BACKWARD ---\n",
      "Processing 4 cached items in reverse:\n",
      "\n",
      "Step 3: Processing activation\n",
      "  Current gradient shape: (4, 1)\n",
      "  Activation backward...\n",
      "    Previous layer input shape: (4, 2)\n",
      "    Activation backward successful, new grad shape: (4, 2)\n",
      "\n",
      "Step 2: Processing layer\n",
      "  Current gradient shape: (4, 2)\n",
      "  Layer backward...\n",
      "    Layer backward FAILED: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jamie\\AppData\\Local\\Temp\\ipykernel_13652\\2334560544.py\", line 51, in debug_sequential_class\n",
      "    model.backward(grad, lr=0.1)\n",
      "  File \"C:\\Users\\jamie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\cvnn\\layers.py\", line 162, in backward\n",
      "    grad = l.backward(grad, lr=lr)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jamie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\cvnn\\layers.py\", line 99, in backward\n",
      "    dx = grad_output @ self.W.T\n",
      "         ~~~~~~~~~~~~^~~~~~~~~~\n",
      "ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Examine Sequential.backward method implementation\n",
    "print(\"\\n\\nStep 4: Investigating Sequential.backward Implementation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Let's look at how the Sequential class handles the backward pass\n",
    "# First, let's recreate the exact scenario that fails\n",
    "\n",
    "def debug_sequential_class():\n",
    "    print(\"Creating Sequential model that fails...\")\n",
    "    \n",
    "    X_fail = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]], dtype=np.float64)\n",
    "    Y_fail = np.array([[0.0], [1.0], [1.0], [0.0]], dtype=np.float64)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(input_dim=2, output_dim=2, real=True, complex=False),\n",
    "        (np.tanh, lambda x, grad: grad * (1 - np.tanh(x)**2)),\n",
    "        Dense(input_dim=2, output_dim=1, real=True, complex=False),\n",
    "        (lambda x: 1/(1+np.exp(-x)), lambda x, grad: grad * (1/(1+np.exp(-x))) * (1 - 1/(1+np.exp(-x))))\n",
    "    ], real=True)\n",
    "    \n",
    "    print(f\"Model layers: {len(model.layers)}\")\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, 'forward'):\n",
    "            print(f\"  Layer {i}: Dense layer with weights {layer.W.shape}\")\n",
    "        else:\n",
    "            print(f\"  Layer {i}: Activation function\")\n",
    "    \n",
    "    # Try one forward pass\n",
    "    print(f\"\\nTrying forward pass...\")\n",
    "    try:\n",
    "        output = model.forward(X_fail)\n",
    "        print(f\"Forward pass successful, output shape: {output.shape}\")\n",
    "        print(f\"Cache length: {len(model.cache)}\")\n",
    "        \n",
    "        for i, (kind, layer_info) in enumerate(model.cache):\n",
    "            print(f\"  Cache {i}: {kind}\")\n",
    "            if kind == \"layer\":\n",
    "                print(f\"    Cached input shape: {layer_info.x_cache.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Forward pass failed: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Now try backward pass manually\n",
    "    print(f\"\\nTrying backward pass...\")\n",
    "    loss = np.mean(np.abs(output - Y_fail) ** 2)\n",
    "    grad = 2 * (output - Y_fail) / Y_fail.shape[0]\n",
    "    print(f\"Initial gradient shape: {grad.shape}\")\n",
    "    \n",
    "    try:\n",
    "        model.backward(grad, lr=0.1)\n",
    "        print(\"Backward pass successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Backward pass failed: {e}\")\n",
    "        import traceback\n",
    "        print(\"Detailed traceback:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Let's debug step by step\n",
    "        print(f\"\\n--- DEBUGGING SEQUENTIAL BACKWARD ---\")\n",
    "        print(f\"Processing {len(model.cache)} cached items in reverse:\")\n",
    "        \n",
    "        current_grad = grad\n",
    "        for i, (kind, l) in enumerate(reversed(model.cache)):\n",
    "            step = len(model.cache) - 1 - i\n",
    "            print(f\"\\nStep {step}: Processing {kind}\")\n",
    "            print(f\"  Current gradient shape: {current_grad.shape}\")\n",
    "            \n",
    "            if kind == \"activation\":\n",
    "                print(f\"  Activation backward...\")\n",
    "                # l is (activation, derivative)\n",
    "                if l[1] is not None:\n",
    "                    # Find the previous layer's cached input\n",
    "                    prev_idx = len(model.cache) - 1 - i - 1\n",
    "                    if prev_idx >= 0:\n",
    "                        prev_kind, prev_layer = model.cache[prev_idx]\n",
    "                        if prev_kind == \"layer\":\n",
    "                            print(f\"    Previous layer input shape: {prev_layer.x_cache.shape}\")\n",
    "                            try:\n",
    "                                current_grad = l[1](prev_layer.x_cache, current_grad)\n",
    "                                print(f\"    Activation backward successful, new grad shape: {current_grad.shape}\")\n",
    "                            except Exception as act_e:\n",
    "                                print(f\"    Activation backward FAILED: {act_e}\")\n",
    "                                break\n",
    "                        else:\n",
    "                            print(\"    ERROR: Previous item is not a layer!\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(\"    ERROR: No previous layer found!\")\n",
    "                        break\n",
    "                else:\n",
    "                    print(\"    ERROR: Activation missing derivative!\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"  Layer backward...\")\n",
    "                try:\n",
    "                    current_grad = l.backward(current_grad, lr=0.1)\n",
    "                    print(f\"    Layer backward successful, new grad shape: {current_grad.shape}\")\n",
    "                except Exception as layer_e:\n",
    "                    print(f\"    Layer backward FAILED: {layer_e}\")\n",
    "                    break\n",
    "\n",
    "debug_sequential_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0990f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 5: Root Cause Analysis and Fix\n",
      "----------------------------------------\n",
      "PROBLEM IDENTIFIED:\n",
      "The Sequential.backward method has a logic error in how it handles activation derivatives.\n",
      "\n",
      "Current (BUGGY) logic:\n",
      "1. Activation backward uses: l[1](prev_layer.x_cache, current_grad)\n",
      "2. But the sigmoid derivative expects: sigmoid_deriv(z2, grad)\n",
      "3. The 'prev_layer.x_cache' is actually 'a1' (the INPUT to the sigmoid)\n",
      "4. But sigmoid derivative needs 'z2' (the PRE-activation input to sigmoid)\n",
      "\n",
      "The bug is that activation derivatives need the PRE-activation values, not the input to the layer.\n",
      "\n",
      "DEMONSTRATION:\n",
      "z1 (layer1 output): (1, 2)\n",
      "a1 (after tanh, layer2 input): (1, 2)\n",
      "z2 (layer2 output): (1, 1)\n",
      "a2 (after sigmoid): (1, 1)\n",
      "\n",
      "CORRECT sigmoid derivative:\n",
      "  Input: z2 (1, 1), grad (1, 1)\n",
      "  Output gradient: (1, 1)\n",
      "\n",
      "INCORRECT computation (current Sequential.backward):\n",
      "  Tries to use: a1 (1, 2) instead of z2 (1, 1)\n",
      "  This causes dimension mismatches!\n",
      "\n",
      "SOLUTION:\n",
      "The Sequential class needs to cache the PRE-activation values (z) for each activation,\n",
      "not just the POST-activation values (a).\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Root Cause Analysis and Fix\n",
    "print(\"\\n\\nStep 5: Root Cause Analysis and Fix\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def analyze_bug():\n",
    "    print(\"PROBLEM IDENTIFIED:\")\n",
    "    print(\"The Sequential.backward method has a logic error in how it handles activation derivatives.\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Current (BUGGY) logic:\")\n",
    "    print(\"1. Activation backward uses: l[1](prev_layer.x_cache, current_grad)\")\n",
    "    print(\"2. But the sigmoid derivative expects: sigmoid_deriv(z2, grad)\")\n",
    "    print(\"3. The 'prev_layer.x_cache' is actually 'a1' (the INPUT to the sigmoid)\")\n",
    "    print(\"4. But sigmoid derivative needs 'z2' (the PRE-activation input to sigmoid)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"The bug is that activation derivatives need the PRE-activation values, not the input to the layer.\")\n",
    "    print()\n",
    "    \n",
    "    # Let's demonstrate the correct vs incorrect computation\n",
    "    print(\"DEMONSTRATION:\")\n",
    "    \n",
    "    # Setup\n",
    "    X = np.array([[1.0, 0.0]], dtype=np.float64)\n",
    "    layer1 = Dense(input_dim=2, output_dim=2, real=True, complex=False)  \n",
    "    layer2 = Dense(input_dim=2, output_dim=1, real=True, complex=False)\n",
    "    \n",
    "    # Forward pass\n",
    "    z1 = layer1.forward(X)      # (1, 2)\n",
    "    a1 = np.tanh(z1)            # (1, 2) - this gets cached as layer2.x_cache\n",
    "    z2 = layer2.forward(a1)     # (1, 1) - this is what sigmoid derivative needs\n",
    "    a2 = 1/(1+np.exp(-z2))      # (1, 1) - final output\n",
    "    \n",
    "    print(f\"z1 (layer1 output): {z1.shape}\")\n",
    "    print(f\"a1 (after tanh, layer2 input): {a1.shape}\")  \n",
    "    print(f\"z2 (layer2 output): {z2.shape}\")\n",
    "    print(f\"a2 (after sigmoid): {a2.shape}\")\n",
    "    \n",
    "    # Gradient from loss\n",
    "    grad_from_loss = np.array([[1.0]])  # (1, 1)\n",
    "    \n",
    "    # CORRECT sigmoid derivative computation\n",
    "    print(f\"\\nCORRECT sigmoid derivative:\")\n",
    "    print(f\"  Input: z2 {z2.shape}, grad {grad_from_loss.shape}\")\n",
    "    sigmoid_out = 1/(1+np.exp(-z2))\n",
    "    correct_grad = grad_from_loss * sigmoid_out * (1 - sigmoid_out)\n",
    "    print(f\"  Output gradient: {correct_grad.shape}\")\n",
    "    \n",
    "    # INCORRECT computation (what Sequential.backward currently does)\n",
    "    print(f\"\\nINCORRECT computation (current Sequential.backward):\")\n",
    "    print(f\"  Tries to use: a1 {a1.shape} instead of z2 {z2.shape}\")\n",
    "    print(f\"  This causes dimension mismatches!\")\n",
    "    \n",
    "    print(f\"\\nSOLUTION:\")\n",
    "    print(\"The Sequential class needs to cache the PRE-activation values (z) for each activation,\")\n",
    "    print(\"not just the POST-activation values (a).\")\n",
    "\n",
    "analyze_bug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e76362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 6: Creating Fixed Sequential Class\n",
      "---------------------------------------------\n",
      "Testing Fixed Sequential Class:\n",
      "✓ Fixed model created successfully\n",
      "Epoch 1/100, Loss: 0.3428\n",
      "Epoch 11/100, Loss: 0.3210\n",
      "Epoch 21/100, Loss: 0.3072\n",
      "Epoch 31/100, Loss: 0.2965\n",
      "Epoch 41/100, Loss: 0.2885\n",
      "Epoch 51/100, Loss: 0.2823\n",
      "Epoch 61/100, Loss: 0.2765\n",
      "Epoch 71/100, Loss: 0.2703\n",
      "Epoch 81/100, Loss: 0.2636\n",
      "Epoch 91/100, Loss: 0.2575\n",
      "Epoch 100/100, Loss: 0.2536\n",
      "✓ Training completed! Final loss: 0.2536\n",
      "✓ Predictions: [0.597 0.531 0.516 0.45 ]\n",
      "✓ Targets:     [0. 1. 1. 0.]\n",
      "✓ Accuracy (±0.3): 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Implement Fixed Sequential Class\n",
    "print(\"\\n\\nStep 6: Creating Fixed Sequential Class\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "class FixedSequential:\n",
    "    \"\"\"Fixed Sequential container that properly handles activation derivatives.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, real=False):\n",
    "        self.layers = []\n",
    "        self.real = real\n",
    "        for l in layers:\n",
    "            if isinstance(l, str) and hasattr(activations, l):\n",
    "                act = getattr(activations, l)\n",
    "                self.layers.append((act, getattr(activations, l + \"_deriv\", None)))\n",
    "            elif isinstance(l, tuple) and len(l) == 2:\n",
    "                self.layers.append(l)\n",
    "            else:\n",
    "                self.layers.append(l)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache = []\n",
    "        current = x\n",
    "        \n",
    "        for l in self.layers:\n",
    "            if hasattr(l, \"forward\"):\n",
    "                # Dense layer\n",
    "                current = l.forward(current)\n",
    "                self.cache.append((\"layer\", l, None))\n",
    "            elif isinstance(l, tuple) and callable(l[0]):\n",
    "                # Activation function - cache the PRE-activation value\n",
    "                pre_activation = current.copy()  # This is the z value\n",
    "                current = l[0](current)\n",
    "                self.cache.append((\"activation\", l, pre_activation))\n",
    "            else:\n",
    "                raise ValueError(\"Unknown layer/activation type\")\n",
    "        \n",
    "        return current\n",
    "    \n",
    "    def backward(self, grad, lr=0.01):\n",
    "        current_grad = grad\n",
    "        \n",
    "        for i, (kind, l, cached_pre_activation) in enumerate(reversed(self.cache)):\n",
    "            if kind == \"activation\":\n",
    "                # l is (activation, derivative)\n",
    "                if l[1] is not None:\n",
    "                    # Use the cached PRE-activation value (z), not the previous layer's input\n",
    "                    current_grad = l[1](cached_pre_activation, current_grad)\n",
    "                else:\n",
    "                    raise ValueError(\"Activation missing derivative\")\n",
    "            else:\n",
    "                # Dense layer\n",
    "                current_grad = l.backward(current_grad, lr=lr)\n",
    "    \n",
    "    def fit(self, x, y, epochs=1000, lr=0.01, verbose=False):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            out = self.forward(x)\n",
    "            loss = np.mean(np.abs(out - y) ** 2)  # MSE loss\n",
    "            grad = 2 * (out - y) / y.shape[0]  # MSE gradient\n",
    "            self.backward(grad, lr=lr)\n",
    "            losses.append(float(loss))\n",
    "            if verbose and (epoch % (epochs // 10) == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        return losses\n",
    "\n",
    "# Test the fixed Sequential class\n",
    "print(\"Testing Fixed Sequential Class:\")\n",
    "\n",
    "X_test = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]], dtype=np.float64)\n",
    "Y_test = np.array([[0.0], [1.0], [1.0], [0.0]], dtype=np.float64)\n",
    "\n",
    "try:\n",
    "    fixed_model = FixedSequential([\n",
    "        Dense(input_dim=2, output_dim=3, real=True, complex=False),\n",
    "        (np.tanh, lambda x, grad: grad * (1 - np.tanh(x)**2)),\n",
    "        Dense(input_dim=3, output_dim=1, real=True, complex=False),\n",
    "        (lambda x: 1/(1+np.exp(-x)), lambda x, grad: grad * (1/(1+np.exp(-x))) * (1 - 1/(1+np.exp(-x))))\n",
    "    ], real=True)\n",
    "    \n",
    "    print(\"✓ Fixed model created successfully\")\n",
    "    \n",
    "    losses = fixed_model.fit(X_test, Y_test, epochs=100, lr=0.5, verbose=True)\n",
    "    print(f\"✓ Training completed! Final loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    # Test predictions\n",
    "    predictions = fixed_model.forward(X_test)\n",
    "    print(f\"✓ Predictions: {predictions.flatten().round(3)}\")\n",
    "    print(f\"✓ Targets:     {Y_test.flatten()}\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.abs(predictions.flatten() - Y_test.flatten()) < 0.3)\n",
    "    print(f\"✓ Accuracy (±0.3): {accuracy:.1%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Fixed model failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706fca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Summary and Recommendations\n",
    "print(\"\\n\\nStep 7: Summary and Recommendations\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "print(\"🔍 PROBLEM SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"The CVNN library's Sequential.fit() function has a matrix dimension bug\")\n",
    "print(\"in the backward pass when handling activation function derivatives.\")\n",
    "print()\n",
    "\n",
    "print(\"🐛 ROOT CAUSE:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Activation derivatives need PRE-activation values (z) as input\")\n",
    "print(\"2. Current implementation passes POST-activation values (previous layer's cached input)\")\n",
    "print(\"3. This causes dimension mismatches when layers have different input/output sizes\")\n",
    "print()\n",
    "\n",
    "print(\"💡 THE FIX:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Cache PRE-activation values for each activation function during forward pass\")\n",
    "print(\"2. Use these cached PRE-activation values in the backward pass\")\n",
    "print(\"3. This ensures proper gradient computation and dimension matching\")\n",
    "print()\n",
    "\n",
    "print(\"📋 WORKAROUNDS FOR NOW:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Option 1: Use the FixedSequential class from this notebook\")\n",
    "print(\"Option 2: Implement training manually with individual Dense layers\")\n",
    "print(\"Option 3: Use single-layer networks without activations\")\n",
    "print(\"Option 4: Wait for the library to be patched\")\n",
    "print()\n",
    "\n",
    "print(\"🚀 NEXT STEPS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Report this bug to the CVNN library maintainers\")\n",
    "print(\"2. Suggest the fix implemented in FixedSequential\")\n",
    "print(\"3. Use manual training loops or FixedSequential for now\")\n",
    "print()\n",
    "\n",
    "# Test comparison: Original vs Fixed\n",
    "print(\"📊 COMPARISON TEST:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simple test case\n",
    "X_simple = np.array([[1.0, 0.0], [0.0, 1.0]], dtype=np.float64)\n",
    "Y_simple = np.array([[1.0], [0.0]], dtype=np.float64)\n",
    "\n",
    "print(\"Testing original Sequential (should fail):\")\n",
    "try:\n",
    "    original_model = Sequential([\n",
    "        Dense(input_dim=2, output_dim=2, real=True, complex=False),\n",
    "        (np.tanh, lambda x, grad: grad * (1 - np.tanh(x)**2)),\n",
    "        Dense(input_dim=2, output_dim=1, real=True, complex=False)\n",
    "    ], real=True)\n",
    "    original_losses = original_model.fit(X_simple, Y_simple, epochs=10, lr=0.1, verbose=False)\n",
    "    print(f\"✓ Original worked (simple case): Final loss {original_losses[-1]:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Original failed: {type(e).__name__}\")\n",
    "\n",
    "print(\"\\nTesting FixedSequential (should work):\")\n",
    "try:\n",
    "    fixed_model = FixedSequential([\n",
    "        Dense(input_dim=2, output_dim=2, real=True, complex=False),\n",
    "        (np.tanh, lambda x, grad: grad * (1 - np.tanh(x)**2)),\n",
    "        Dense(input_dim=2, output_dim=1, real=True, complex=False)\n",
    "    ], real=True)\n",
    "    fixed_losses = fixed_model.fit(X_simple, Y_simple, epochs=10, lr=0.1, verbose=False)\n",
    "    print(f\"✓ Fixed worked: Final loss {fixed_losses[-1]:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Fixed failed: {type(e).__name__}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 CONCLUSION: Matrix dimension issues in CVNN Sequential.fit() are\")\n",
    "print(\"    caused by incorrect handling of activation derivatives.\")\n",
    "print(\"    Use FixedSequential class or manual training for reliable results.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
